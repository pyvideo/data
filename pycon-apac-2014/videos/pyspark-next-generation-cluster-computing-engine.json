{
  "alias": "video/3273/pyspark-next-generation-cluster-computing-engine",
  "category": "PyCon APAC 2014",
  "copyright_text": "youtube",
  "description": "Apache Spark\u2122 is a lightning fast engine for large-scale data\nprocessing. It is an in-memory cluster computing framework, originally\ndeveloped in UC Berkeley. Base on it's project page's evaluation,\nmachine learning programming can run program 100x faster than Hadoop\nMapReduce. And Spark can run on Hadoop 2's YARN cluster manager, and can\nread any existing Hadoop data. Currently, it supports Scala, Java and\nPython for writing spark programs.\n\nIn this talk, I will introduce the General concept of Spark's\ninfrastructure, What is RDD (Resilient Distributed Datasets) in Spark,\nIntroduction on PySpark, Demo of PySpark's speed and power, Head-to-head\ncomparison between two programs doing same work - one written in Hadoop\nMapReduce and the other written using PySpark.\n\nI will also conclude about the companies currently using Spark's use\ncases.\n\nAbout the speaker\n\nSr. Software Engineer for the Yahoo! (Taiwan) Data Team. He has been\nresponsible for data infrastructure, data solution, software release and\ncontinuous integration management. He is a lifelong student of software\ndevelopment/testing/deployment/CI processes and best practices and an\navid coding puzzle competition fanatic as well as Open Source evangelist\n",
  "duration": null,
  "id": 3273,
  "language": "eng",
  "quality_notes": "",
  "recorded": "2014-06-24",
  "slug": "pyspark-next-generation-cluster-computing-engine",
  "speakers": [
    "Wisely Chen"
  ],
  "summary": "",
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/gaR7svYX8pw/hqdefault.jpg",
  "title": "PySpark: next generation cluster computing engine",
  "videos": [
    {
      "length": 0,
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=gaR7svYX8pw"
    }
  ]
}