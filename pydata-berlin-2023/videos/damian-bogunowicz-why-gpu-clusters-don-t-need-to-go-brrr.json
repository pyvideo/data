{
  "description": "Forget specialized hardware. Get GPU-class performance on your commodity CPUs with compound sparsity and sparsity-aware inference execution.\nThis talk will demonstrate the power of compound sparsity for model compression and inference speedup for NLP and CV domains, with a special focus on the recently popular Large Language Models. The combination of structured + unstructured pruning (to 90%+ sparsity), quantization, and knowledge distillation can be used to create models that run an order of magnitude faster than their dense counterparts, without a noticeable drop in accuracy. The session participants will learn the theory behind compound sparsity, state-of-the-art techniques, and how to apply it in practice using the Neural Magic platform.",
  "duration": 2567,
  "language": "eng",
  "recorded": "2023-04-17",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://2023.pycon.de/"
    }
  ],
  "speakers": [
    "TODO"
  ],
  "tags": [
    "Education",
    "Julia",
    "NumFOCUS",
    "Opensource",
    "PyData",
    "Python",
    "Tutorial",
    "coding",
    "how to program",
    "learn",
    "learn to code",
    "python 3",
    "scientific programming",
    "software"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/YtsW-VPqEnA/maxresdefault.jpg",
  "title": "Damian Bogunowicz: Why GPU Clusters Don't Need to Go Brrr?",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=YtsW-VPqEnA"
    }
  ]
}
