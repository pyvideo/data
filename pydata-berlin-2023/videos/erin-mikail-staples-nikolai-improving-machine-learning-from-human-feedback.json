{
  "description": "Large generative models rely upon massive data sets that are collected automatically. For example, GPT-3 was trained with data from \"Common Crawl\" and \"Web Text\", among other sources. As the saying goes — bigger isn't always better. While powerful, these data sets (and the models that they create) often come at a cost, bringing their \"internet-scale biases\" along with their \"internet-trained models.\" While powerful, these models beg the question — is unsupervised learning the best future for machine learning?\n\nML researchers have developed new model-tuning techniques to address the known biases within existing models and improve their performance (as measured by response preference, truthfulness, toxicity, and result generalization). All of this at a fraction of the initial training cost. In this talk, we will explore these techniques, known as Reinforcement Learning from Human Feedback (RLHF), and how open-source machine learning tools like PyTorch and Label Studio can be used to tune off-the-shelf models using direct human feedback.",
  "duration": 1744,
  "language": "eng",
  "recorded": "2023-04-17",
  "related_urls": [
    {
      "label": "Talk Page",
      "url": "https://2023.pycon.de/program/AUJYP7/"
    }
  ],
  "speakers": [
    "Erin Mikail Staples",
    "Nikolai Liubimov"
  ],
  "tags": [
    "reinforcement learning",
    "human feedback",
    "pytorch",
    "label studio"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/IbuNQsM9uAA/maxresdefault.jpg",
  "title": "Improving Machine Learning from Human Feedback",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=IbuNQsM9uAA"
    }
  ]
}
