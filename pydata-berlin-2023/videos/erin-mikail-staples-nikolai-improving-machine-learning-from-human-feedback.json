{
  "description": "Large generative models rely upon massive data sets that are collected automatically. For example, GPT-3 was trained with data from \u201cCommon Crawl\u201d and \u201cWeb Text\u201d, among other sources. As the saying goes \u2014 bigger isn\u2019t always better. While powerful, these data sets (and the models that they create) often come at a cost, bringing their \u201cinternet-scale biases\u201d along with their \u201cinternet-trained models.\u201d While powerful, these models beg the question \u2014 is unsupervised learning the best future for machine learning?\n\nML researchers have developed new model-tuning techniques to address the known biases within existing models and improve their performance (as measured by response preference, truthfulness, toxicity, and result generalization). All of this at a fraction of the initial training cost. In this talk, we will explore these techniques, known as Reinforcement Learning from Human Feedback (RLHF), and how open-source machine learning tools like PyTorch and Label Studio can be used to tune off-the-shelf models using direct human feedback.",
  "duration": 1744,
  "language": "eng",
  "recorded": "2023-04-17",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://2023.pycon.de/"
    }
  ],
  "speakers": [
    "TODO"
  ],
  "tags": [
    "Education",
    "Julia",
    "NumFOCUS",
    "Opensource",
    "PyData",
    "Python",
    "Tutorial",
    "coding",
    "how to program",
    "learn",
    "learn to code",
    "python 3",
    "scientific programming",
    "software"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/IbuNQsM9uAA/maxresdefault.jpg",
  "title": "Erin Mikail Staples, Nikolai: Improving Machine Learning from Human Feedback",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=IbuNQsM9uAA"
    }
  ]
}
