{"whiteboard": "", "video_mp4_download_only": false, "video_webm_download_only": false, "duration": null, "video_ogv_download_only": false, "category": "EuroPython 2014", "speakers": ["Jozef van Eenbergen"], "title": "Amanda: A New Generation of Distributed Services Framework", "quality_notes": "", "video_flv_length": null, "recorded": "2014-07-22", "video_mp4_length": null, "description": "Presentation outline\r\n====================\r\n\r\nWe'll start off with a quick overview of a movie production pipeline which will set the stage\r\nfor how Amanda provides artists with the tools they need to develop and streamline the production process\r\nas well as Amanda's crucial function as a robust framework for the support and development teams.\r\nGoing over some stats, up to 250.000 service calls a minute during World War Z for example (for frame of reference this is\r\ntwice the average rate of stackoverflow.com), I'll highlight some of the problems encountered with the 1st generation.\r\nInitially developed in 2007 and replaced last year it had several flaws in regards to scalability, maintainability and future proofing.\r\nFrom there I'll introduce the 2nd generation which is build on the principle of componentisation and building blocks. Every part of the system\r\nneeds to be replaceable and this needs to be possible from the configuration.\r\n\r\nDuring the presentation we will be stepping through the different building blocks, how they have been set up, how they slot together\r\nand how we monitor, trace and test the system from the ground up. Starting at the lowest level with services we'll slowly\r\nstep through the different blocks necessary to build a fault tolerant, distributed and scalable platform.\r\nWe made sure that the platform is not tied into any specific technology but allows the use of the best technologies\r\ndepending on the type of work being undertaken and changing business needs and technological advances.\r\n\r\nService development and testing\r\n-------------------------------\r\n\r\nOur development teams build applications for artists creating visual effects through to management teams coordinating productions.\r\nA service-based architecture was chosen to provide consistent interfaces across the many different environments where this is required.\r\nWe provide an ecosystem where developers of any level can safely write a service (a set of instructions regarding a\r\nspecific topic) that are presented to developers and technical artists globally.\r\nTo write a service the developer doesn't need any knowledge in regards to building large concurrent systems.\r\nThe service is implemented through a simple Python API and the provided ecosystem allows services to exist in a standalone manner.\r\nThe service concept was separated from the platform hosting it. This allows hosting in any application that provides a\r\nstandard container (a service provider). Extracting this allowed for more rigorous and simple testing of services;\r\nit also allows developers to provide fake versions of their services publicly against which client code can be tested.\r\nThe adage \u00ca\u00bbeverything as a service\u00ca\u00bc was applied to the development of internal facilities.\r\nThis includes our management tools and the developer console, which presents the documentation of services and methods\r\navailable to developers through a web interface.\r\nInfrastructure services were introduced to present an interface to facilities provided to a regular service, for example\r\ndatabases, configuration and centralized logging.\r\nServices can call other services and, similarly to infrastructures, services can be replaced with different services depending on the configuration.\r\nServices are exposed to a service (or client as we will see later) via a service provider just like in applications.\r\nSetting services up with the above patterns allows developers to iterate quickly and to include services within testing frameworks.\r\nIt has also provided a standardized form across projects allowing developers to support and add to unfamiliar code easily.\r\nAnd last but not least it has given us full abstractions at all levels, users of services do not need to know the code underneath the hood\r\nbe it at a service level or at an infrastructure level.\r\n\r\n\r\nBuilding the cluster\r\n--------------------\r\n\r\nRather than building a single system, the new architecture defines a set of building blocks for constructing a\r\ndistributed service platform. These can provide adapters for best of breed third party tools or, where necessary,\r\ncustom implementations of functionality. Configuration is used to determine the number and types of modules to use\r\nand the parameters with which to initialize them. This allows the same platform to be used for small instances at\r\na developer\u00ca\u00bcs desk up to a production environment of many nodes. The design enables improved components to be\r\nswapped into the existing system whilst forming the basis for an entirely new design.\r\n\r\nMost practical applications require the service provider to handle multiple requests at the same time.\r\nAmanda provides a set of interchangeable concurrency modules. This allows the most appropriate Python model\r\nfor parallel processing to be chosen. For work involving heavy I/O work we choose approaches that avoid waiting\r\nfor the GIL, for example multiple processes and greenlets/coroutines, whilst for CPU bound work we can use threads\r\nwhich may prove more performant.  Having the option to choose between mechanisms is important since there is not a\r\nsolution that neatly fits all use cases. A pluggable concurrency abstraction also allows integration of new libraries\r\nas they become available. In future this might include the new asyncio (formerly Tulip) core library for Python 3.3+.\r\n\r\nTo benefit from concurrency, resource pooling, caching etc. we don't always want to execute the service locally to the service provider.\r\nService proxies implement this behavior; they take the service, method and arguments of a request as their input\r\nand return the result. The proxy should be transparent to the service and service provider components. By chaining\r\nproxies, complex routing schemes can be built or analysis performed on the results returned. Some similarity can be\r\ndrawn with middle-ware in the Web Services Gateway Interface (WSGI) specification.\r\nCommunication between proxy and service provider is served by the transport. This abstraction provides an\r\nasynchronous interface to underlying technologies \u00e2\u20ac\u201c Current implementations include queue based AMQP, ontop of\r\nRabbitMQ, and \u00c3\u02dcMQ and more na\u00c3\u00afve communications with standard UDP and TCP sockets. Most transports define both client\r\nand server parts of the system \u00e2\u20ac\u201c however some, particularly HTTP-based transports, are designed to accept requests directly from external clients.\r\nRequests from external applications commonly use XMLRPC, JSONRPC or straight JSON. Transport implementations can be\r\ninterchanged without impacting other components of Amanda or service developers.\r\n\r\nIn production, a request gateway implemented as a WSGI application fronts the HTTP protocols. Using the standard\r\nweb components NGINX and \u00ce\u00bcWSGI we can build a very scalable front end which internally uses the service provider, proxy, transport\r\npattern to offload the requests to a backend. The gateway can also provide standard web facilities such as template rendering\r\n(through the Jinja2 library1) for general web clients. The gateway was a requirement as requests originate from applications\r\nwritten in many languages including C++, Python, JavaScript and domain specific languages such as mel. For us it was\r\nimportant that the client used across all those languages was a proven standard and lightweight. Most requests are served\r\nin near realtime (6ms round trip times) and are presented to the client in a synchronous way so using a frontend that supports a large number\r\nof HTTP like protocols allowed us to keep the clients simple and present the platform to an extremely wide variety of\r\nlanguages. Additionally, through the frontend, we can render a web page and present that directly if the requests was made\r\nfrom a browser.\r\n\r\nThe final behavior of the platform is defined in configuration. This allows the platform to be tuned to suit\r\nthe work that a particular service is performing (I/O vs CPU bound). It is important to remember that every single\r\ncomponent mentioned above be it the concurrency, transport, proxies or frontend can be changed, removed, updated without\r\nit impacting the service, the developer or any of the other components that make up the platform.\r\n\r\nAlso important to mention that internally and externally everything is a queue and presented as a queue. Going from the client\r\nto the frontend there is a queue, from the frontend onto the backend there is a queue etc. all the way down to a request\r\nbeing read of the transport and stored inside a queue until a concurrency object is ready to handle the request with the\r\nservice provider.\r\n\r\nThis is where we think our platform might take a different approach. Rather than building the platform on top of a single\r\ngreat technology we didn't want to limit ourselves and be able to use all the other great technologies out there.\r\nThere is no perfect solution for all problems but allowing to fine tune the platform according to different problems.\r\nThe setup can now evolve in line with technological advancements and changes to the industry.\r\n\r\n\r\nMaintenance and Monitoring (5 mins)\r\n-----------------------------------\r\n\r\nWe will walk through how we are using the same setup with services, service providers, proxies and transports to manage\r\nclusters around the globe. Once again for our maintenance and monitoring we made sure everything is done as a service so\r\nthat if there is a better tool in the future we could adopt it.\r\n\r\nThrough leveraging the configuration management and remote execute platform Salt, a new cluster can now be provisioned quickly.\r\nManagement is itself provided as a service. Through this system, the current state is available and configuration changed across\r\nall servers globally. This has reduced routine maintenance tasks from a half day to a five-minute task, with less\r\nchance of human error. Monitoring and introspection are provided, as a service, to aid in day-to-day support, tuning and to help\r\nsupport analysis for future development.\r\n\r\nDevelopers of services can trace requests from when they enter the system, producing a report of the sequence of\r\nmethods being called, with the supplied arguments. For each call the time spent to fulfill each request is presented.\r\nCare was taken to minimize the impact of this on return result of the request. Due to everything being a queue we\r\ncan collect the metrics after the result has been put back onto the transport and send to the user and thus minimize the impact\r\nof this collection on returning the result of the request\r\nThis means that there is no requirement to put the system into a debug mode in order to obtain execution metrics.\r\n\r\nWith logging being a service we can dynamically change the logging configuration on a per service basis by making a\r\nrequest to the logging service taking away the need of changing configuration and restarting the service which\r\noften means a problem might have disappeared due to the reset.\r\n\r\nFuture/Conclusion (1 min)\r\n-------------------------\r\n\r\nWhilst developing the new generation of the platform there have been a number of possible applications that have\r\nemerged. The way in which we are able to scale the system would be suitable to run in a cloud environment \u00e2\u20ac\u201c\r\n especially with the improvements to management allowing new nodes to be provisioned quickly. The ease of writing\r\nand integrating new components would allow integration with infrastructure provided by third-party cloud vendors.\r\nOther areas of interest include a smaller version of the platform running locally on a user\u00ca\u00bcs workstation and\r\nservices for management of generic processes.\r\n\r\nMain technologies and libraries currently used:\r\n------------------------------------------\r\n\r\n* Threading\r\n* Gevent\r\n* Eventlet\r\n* Multiprocessing\r\n* ZeroMQ\r\n* RabbitMQ\r\n* uwsgi\r\n* Flask\r\n* Salt\r\n* nginx\r\n", "video_mp4_url": "", "tags": [], "copyright_text": "http://creativecommons.org/licenses/by/3.0/", "related_urls": [], "video_flv_download_only": false, "source_url": "http://www.youtube.com/watch?v=bPNGn1XHCKw", "video_webm_url": "", "video_ogv_length": null, "video_ogv_url": "", "language": "English", "video_webm_length": null, "summary": "To help create award winning visual effects, MPC developed a distributed service-oriented platform, Amanda.\r\nAmanda allows developers of any level to write a service that is presented to users across 8 facilities globally without them requiring any\r\nknowledge of building large concurrent systems. It allows artists and developers across different domains to work with\r\nclearly defined API's and gives the service developer control over what and how data can and should be accessed. The talk will cover how to set up such a platform from the ground up. Starting at the service level building it out with\r\nadditional modules and technologies until the fully distributed system, covering topics such as concurrency, componetisation and monitoring that allow the\r\nfine tuning of setups depending on the type of work being undertaken and changing business needs.", "thumbnail_url": "http://i.ytimg.com/vi/bPNGn1XHCKw/hqdefault.jpg", "video_flv_url": ""}