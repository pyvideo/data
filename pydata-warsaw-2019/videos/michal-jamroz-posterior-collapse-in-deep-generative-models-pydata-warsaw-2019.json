{
  "description": "Generative models are powerful Machine Learning models useful at\nextracting information from high-dimensional data, but they sometimes\nsuffer from the problem called \"posterior collapse\" which prevents them\nfrom learning representation having practical value. I am going to show\nwhy and when it happens, also how to deal with it.\n\n**Why**\n\nDeep generative models like Variational AutoEncoders (VAEs) and\nGenerative Adversarial Networks (GANs) turned out to be very successful\nin real-world applications of machine learning, including: natural image\nmodelling, data compression, audio synthesis and many more.\nUnfortunately, it appears that models belonging to VAEs family - under\nsome conditions may suffer from an undesired phenomenon called\n\"posterior collapse\" which causes them to learn poor data\nrepresentation. The talk's purpose is to present this problem and its\npractical implications.\n\n**What**\n\nThe presentation will comprise following elements:\n\n-  Short introduction of basic Variational AutoEncoder model\n-  Introducing the \"posterior collapse\" problem\n-  How posterior collapse affects learning from data - natural images\n   examples\n-  Some research on dealing with posterior collapse\n\n**Audience**\n\nBeing familiarised with the topic of generative modelling will be\nhelpful for anyone attending the talk, but it's not required. In fact,\neveryone having basic understanding of neural networks, representation\nlearning and probability can gain useful information. Presentation won't\nbe overloaded with mathematical formulas, I will do my best to present\nmath-related aspects in an intuitive form.\n",
  "duration": 1669,
  "language": "eng",
  "recorded": "2019-12-12",
  "speakers": [
    "Micha\u0142 Jamro\u017c"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/oHtqlRIsXcQ/hqdefault.jpg",
  "title": "Posterior Collapse in Deep Generative models",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=oHtqlRIsXcQ"
    }
  ]
}
