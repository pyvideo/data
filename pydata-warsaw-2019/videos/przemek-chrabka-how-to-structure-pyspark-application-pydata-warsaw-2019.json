{
  "description": "A lot of the Data Scientists and Engineers don\u2019t come from Software\nEngineering background and even they have an experience with writing\nspark code they might luck the knowledge about application structure\nprincipals. This talk is designed to help them write better and more\nreadable code.\n\nPySpark has become really popular for last couple of years and is now a\ngo-to tool for building and managing data-heavy applications. One of the\nmost common ways how Spark is used is moving some data around by writing\nETL/ELT jobs. Doing that your code should be manageable and\nunderstandable to others. In this talk I will try to introduce good\npractice how to structure PySpark application and write jobs and also\nsome naming conventions.\n\nI will start this talk with an example of bad way of writing PySpark job\nand during the course of it we will gradually improve it so at the end\nour application is going to be production ready, easy to manage and\nshare with other developers.\n\nDuring this talk I will try to answer this questions: - How to structure\nPySpark ETL application - How to write ETL job - How to package your\ncode and dependencies - What are some coding and naming conventions\n",
  "duration": 1788,
  "language": "eng",
  "recorded": "2019-12-12",
  "speakers": [
    "Przemek Chrabka"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/Bp0XvA3wIXw/hqdefault.jpg",
  "title": "How to structure PySpark application",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=Bp0XvA3wIXw"
    }
  ]
}
