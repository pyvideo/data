{
  "description": "Imagine you need to choose ten articles out of hundreds in a way that\nmaximizes your profit. It's not as easy as it seems. In this talk, we\nwill explain how we prepare recommendations on the onet.pl home page for\nmillions of users with the use of a multi-armed bandit algorithm.\n\nMulti-armed bandits are a powerful solution for a diversity of\noptimization problems that demand a balance between using existing\nknowledge about item performance and acquiring new one. That's why we\nwould like to focus on the intuition behind the multi-armed bandit\napproach and its application in recommender systems on the example of\nonet.pl home page. Also, we will introduce E-greedy, UCB and Thompson\nSampling bandits, discuss their pros and cons and show how to tune them\nin a simulated environment.\n",
  "duration": 1610,
  "published_at": "2020-01-03T08:00:08.000Z",
  "recorded": "2019-12-13",
  "speakers": [
    "Artur Bujak",
    "Dominik Rusiecki"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/Loe3D37UHlI/hqdefault.jpg",
  "title": "How we personalized onet.pl with multi-armed bandits",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=Loe3D37UHlI"
    }
  ]
}
