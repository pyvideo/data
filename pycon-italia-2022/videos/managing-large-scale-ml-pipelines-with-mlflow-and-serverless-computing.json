{
  "copyright_text": null,
  "description": "Managing large-scale Machine Learning pipelines with MLflow and\nserverless computing. - PyCon Italia 2022\n\nMLOps aims to manage the machine learning (ML) lifecycle including\nexperimentation, reproducibility, deployment, and model registry. Come\nto discover how in Vedrai - one of the top AI startups in Europe - we\nenhance and maintain ML pipelines models in production reliably and\nefficiently using MLOps. Problem:\n\nOne difficulty of employing Machine Learning (ML) within organizations\nis managing the model\u2019s lifecycle. Moving from experimenting to\ndeployment in production environments is operated by different steps:\nPreparing and Analysing Data, Training, Deployment, Monitoring, and\nGovernance of ML models. So, it is crucial to possess a platform to\nmanage and organize the ML lifecycle.\n\nSolution:\n\nIn Vedrai, we combined the strength of the MLflow framework and the\nresilience of AWS serverless services to manage, deploy, and scale our\nML models in production. MLflow is an open-source framework for tracking\nthe entire ML lifecycle from training to deployment. Among the\nfunctions, it offers model tracking, packaging, and serving. Whereas,\ndeploying ML applications is an infrastructure affair that needs to be\nscalable with minimum server management, which makes AWS serverless\nservices a great choice.\n\nValue:\n\nMLflow enforces the model\u2019s reproducibility and robustness at the same\ntime allowing more centralized experimentation. AWS serverless services\nallow training and inferencing pipelines to run without provisioning or\nmanaging servers while only paying for the time it takes to run.\n\nSummary:\n\n-  State of the art of MLOps.\n-  Record and query experiments with MLflow Tracking.\n-  Package data science code with MLflow Projects.\n-  Store ML models with MLflow Models Registry.\n-  Deploy ML models in the AWS environment.\n-  Future MLOps challenges.\n\nSpeaker: ilyas chaoua\n",
  "duration": 1818,
  "language": "eng",
  "recorded": "2022-06-03",
  "speakers": [
    "ilyas chaoua"
  ],
  "tags": [
    "architecture",
    "aws",
    "best practice",
    "deep learning",
    "devops",
    "docker",
    "infrastructure",
    "machine learning",
    "open source",
    "operations",
    "packaging",
    "performance",
    "scaling"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/8AkqF99tkXg/maxresdefault.jpg",
  "title": "Managing large-scale ML pipelines with MLflow and serverless computing.",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=8AkqF99tkXg"
    }
  ]
}