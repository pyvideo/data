{
  "description": "[EuroPython 2024 \u2014 North Hall on 2024-07-10]\n\n\rDeconstructing the text embedding models by Kacper \u0141ukawski\n\rhttps://ep2024.europython.eu/session/deconstructing-the-text-embedding-models\n\n\rSelecting the optimal text embedding model is often guided by benchmarks such as the Massive Text Embedding Benchmark (MTEB). While choosing the best model from the leaderboard is a common practice, it may not always align perfectly with the unique characteristics of your specific dataset. This approach overlooks a crucial yet frequently underestimated element - the tokenizer.\n\n\rWe will delve deep into the tokenizer's fundamental role, shedding light on its operations and introducing straightforward techniques to assess whether a particular model is suited to your data based solely on its tokenizer. We will explore the significance of the tokenizer in the fine-tuning process of embedding models and discuss strategic approaches to optimize its effectiveness.\n\n\n\r---\n\rThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License: https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "duration": 2666,
  "language": "eng",
  "recorded": "2024-07-08",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://ep2024.europython.eu/"
    },
    {
      "label": "https://creativecommons.org/licenses/by-nc-sa/4.0/",
      "url": "https://creativecommons.org/licenses/by-nc-sa/4.0/"
    },
    {
      "label": "https://ep2024.europython.eu/session/deconstructing-the-text-embedding-models",
      "url": "https://ep2024.europython.eu/session/deconstructing-the-text-embedding-models"
    }
  ],
  "speakers": [
    "Kacper ≈Åukawski"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/Lha_QCkXxm0/maxresdefault.jpg",
  "title": "Deconstructing the text embedding models",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=Lha_QCkXxm0"
    }
  ]
}
