{
  "description": "Lightning Talk: Sparsifying Vision Transformers with Minimal Accuracy Loss - Jesse Cai, Meta\n\nSparsity, like quantization, is an approximate model  optimization technique, where we trade some model accuracy for increased performance.\n\nIn this talk we'll explore how to minimize the accuracy degradation of sparsifying Vision Transformer (ViT) based models to GPU accelerable sparsity patterns like block sparsity and semi-structured sparsity.\n\nWe'll cover the best techniques to ensure a less-than 5% loss in accuracy when:\n- training a sparse model from scratch\n- pruning and retraining an existing dense model\n- zero-shot/one-shot pruning a dense model\n\nWe've collected these techniques into a single repository, torchao, so that model optimization enthusiasts like you can sparsify your models with just a few lines of code.",
  "duration": 841,
  "language": "eng",
  "recorded": "2024-09-18",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://pytorch.org/event/pytorch-conference-2024/"
    }
  ],
  "speakers": [
    "TODO"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi_webp/mqoIfs5hTxA/maxresdefault.webp",
  "title": "Lightning Talk: Sparsifying Vision Transformers with Minimal Accuracy Loss - Jesse Cai, Meta",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=mqoIfs5hTxA"
    }
  ]
}
