{
  "description": "Activation checkpointing is a commonly used technique to reduce memory usage during model training by reducing the number of activations saved for backward. Instead of keeping tensors needed for backward alive until they are used in gradient computation during backward, those tensors are recomputed during the backward pass. This talk will introduce new activation checkpoint APIs that can help achieve a better trade off between memory savings and compute overhead that recomputing introduces.",
  "duration": 750,
  "language": "eng",
  "recorded": "2024-09-18",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://pytorch.org/event/pytorch-conference-2024/"
    }
  ],
  "speakers": [
    "Jeffrey Wan",
    "Horace He"
  ],
  "tags": [
    "Lightning Talk"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi_webp/v3gsrJtGLiA/maxresdefault.webp",
  "title": "New Activation Checkpointing APIs in PyTorch",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=v3gsrJtGLiA"
    }
  ]
}
