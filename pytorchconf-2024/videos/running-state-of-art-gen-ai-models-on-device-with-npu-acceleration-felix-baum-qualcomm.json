{
  "description": "Running State-of-Art Gen AI Models on-Device with NPU Acceleration - Felix Baum, Qualcomm\n\nSince the boom of generative AI, the industry is now moving towards on-device AI inferencing, as it is not only a trend but a necessity now in order to save costs, achieve the best inference performance, ultra-low latency at the lowest power possible. In this session we go over the new features added on the Qualcomm AI Stack and how it works with the public release of ExecuTorch 1.0. We will discuss how to run traditional workloads as well as GenAI use cases including the latest version of Llama on the Mobile device while using Qualcomm Hexagon NPU.",
  "duration": 1460,
  "language": "eng",
  "recorded": "2024-09-18",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://pytorch.org/event/pytorch-conference-2024/"
    }
  ],
  "speakers": [
    "TODO"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi_webp/wd57g2IM3C4/maxresdefault.webp",
  "title": "Running State-of-Art Gen AI Models on-Device with NPU Acceleration - Felix Baum, Qualcomm",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=wd57g2IM3C4"
    }
  ]
}
