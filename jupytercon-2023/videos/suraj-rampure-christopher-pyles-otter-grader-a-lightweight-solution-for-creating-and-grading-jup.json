{
  "description": "Otter-Grader is a lightweight open-source command-line tool for developing and grading Jupyter Notebook assignments at scale. It enables instructors to produce an assignment and its autograder from just a single notebook.\n\nOtter was developed by Christopher Pyles, while working with Data Science Undergraduate Studies at UC Berkeley. Since its pilot in 2020, Otter has been adopted by instructors at a wide variety of institutions, from a university in Japan to a high school in North Carolina, and has been deployed in courses with enrollments ranging from 15 to 1500+.\n\nAttendees will find our talk particularly useful if they\u2019ve created notebooks for educational purposes, and/or if they\u2019ve worked with grading infrastructure such as nbgrader or Gradescope.\n\nPart 1: Authoring Assignments\nWe\u2019ll start by demonstrating how to author assignment notebooks in Python using Otter.\n\nOne of the reasons Otter is so convenient is that an entire assignment and autograder can be developed in just a single \u201csource\u201d notebook. That notebook consists of exposition, solution code that students need to produce, inline autograder tests, and other metadata. After creating a source notebook, a single use of the otter assign command-line tool produces a student-facing version of the notebook. In this notebook, students only see the skeleton code their instructor wants them to start with (rather than the solution), and instead of seeing the nitty-gritty details of all autograder tests, they only see calls to the function grader.check, which displays the test cases that their code for a given question failed.\n\nPart 2: Releasing and Collecting Assignments\nIn addition to creating a student-facing assignment notebook, otter assign also generates a portable autograder.zip file that instructors can run to compute grades. This autograder can be run anywhere that pip install otter-grader can be run \u2013 most commonly, this is in a Docker container on a personal computer or on Gradescope, a popular LMS.\n\nWe will demonstrate a common workflow in use at multiple institutions, which involves:\nHosting student-facing notebooks on GitHub.\nProviding students with nbgitpuller links that open the relevant assignment on an institution-hosted JupyterHub server.\nConfiguring Gradescope to automatically run all autograder tests and provide feedback upon submission (or, alternatively, autograding submissions locally in a Docker container).\n\nAll of this will be illustrated from the perspective of both an instructor and a student.\n\nPart 3: Adaptations, Shortcomings, and Future Plans\nA common data science workflow is to use notebooks for exploration, but to write permanent code using an IDE. In one of our courses, we promote this workflow by distributing assignments as notebooks containing the question prompts, while requiring students to submit their work in .py files. To support this use case, we wrote a wrapper around Otter which takes a notebook containing all problem descriptions, solutions, and test code, and generates a student-facing .py file containing skeleton function definitions. Our wrapper allows us to generate our multi-format assignments using a single source document, thereby significantly reducing the likelihood of errors. We will start the third part of the talk by discussing the motivation behind this type of assignment, how we used nbconvert to support the adaptation, and how much easier this adaptation of Otter makes it to create and edit these assignments than the prior pre-Otter solution.\n\nThen, more broadly, we will discuss shortcomings of Otter that have been identified by other instructors. Some shortcomings are pedagogical:\n\nOne may argue that Otter\u2019s presentation of test cases encourages students to \u201cguess and check\u201d their work.\nAdditionally, depending on the domain, it can be difficult to craft autograder tests when students\u2019 implementations vary significantly. (For instance, in cases where random sampling is involved.)\nOther shortcomings are infrastructural:\n- The current Otter metadata syntax isn\u2019t supported in third-party platforms like Google Colab.\n- Otter does not support question randomization in any way, e.g. it can\u2019t create \u201cversions\u201d of assignments with questions in different orders, which can limit its usefulness for exams.\n\nTo conclude, we will summarize recent updates made to Otter and discuss planned future directions, including how we plan to address some of the aforementioned shortcomings.",
  "duration": 1743,
  "language": "eng",
  "recorded": "2023-05-10",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://web.archive.org/web/20230531110007/https://www.jupytercon.com/"
    }
  ],
  "speakers": [
    "TODO"
  ],
  "tags": [
    "#AIinAction",
    "#CodingInJupyter",
    "#DataAnalysis",
    "#DataDrivenInsights",
    "#DataScienceInnovation",
    "#DataVisualization",
    "#InteractiveComputing",
    "#JupyterCommunity",
    "#JupyterCon2023",
    "#JupyterLove",
    "#MachineLearning",
    "#NotebookWorkflow",
    "#OpenSourceTools",
    "#PythonProgramming",
    "#ResearchTools"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/9_x532_2T2w/maxresdefault.jpg",
  "title": "Suraj Rampure, Christopher Pyles   Otter Grader: A Lightweight Solution for Creating and Grading Jup",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=9_x532_2T2w"
    }
  ]
}
