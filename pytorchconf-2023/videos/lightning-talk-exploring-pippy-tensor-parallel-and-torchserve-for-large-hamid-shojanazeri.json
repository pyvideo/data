{
  "description": "Here, we talk about large model inference with Torchserve, using PiPPy, Tensor Parallel, challenges of distributed inference and available solutions. Discuss the features that Torchserve provide today for serving LLMs in production today.",
  "duration": 753,
  "language": "eng",
  "recorded": "2023-10-16",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://pytorch.org/event/pytorch-conference-2023/"
    }
  ],
  "speakers": [
    "Hamid Shojanazeri"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/QNQwbK2ZSFU/maxresdefault.jpg",
  "title": "Lightning Talk: Exploring PiPPY, Tensor Parallel and Torchserve for Large...",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=QNQwbK2ZSFU"
    }
  ]
}
