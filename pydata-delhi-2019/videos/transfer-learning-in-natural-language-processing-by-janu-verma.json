{
  "description": "Transfer learning refers to the methods that leverage a trained model in\none domain to achieve better results on tasks in a related domain i.e.\nwe transfer the knowledge gained in one domain to a new domain. This\ntalk is centered on recent developments in deep learning to facilitate\ntransfer learning in NLP. We will discuss the transformer architecture\nand its extensions like GPT and BERT.\n\nThe classic supervised machine learning paradigm is based on learning in\nisolation a single predictive model for a task using a single dataset.\nThis approach requires a large number of training examples and performs\nbest for well-defined and narrow tasks. Transfer learning refers to the\nmethods that leverage a trained model in one domain to achieve better\nresults on tasks in a related domain. The model thus trained also show\nbetter generalization properties.\n\nComputer vision has seen great success of transfer learning, model\ntrained on the Imagenet data have been 'fine-tuned' to achieve\nState-of-thee-art in many other problems. In last two years, NLP has\nalso witnessed the emergence of several transfer learning methods and\narchitectures, which significantly improved upon the state-of-the-art on\na wide range of NLP tasks.\n\nWe will present an overview of modern transfer learning methods in NLP,\nhow models are pre-trained, what information the representations they\nlearn capture, and review examples and case studies on how these models\ncan be integrated and adapted in downstream NLP tasks.\n",
  "duration": 2256,
  "language": "eng",
  "recorded": "2019-08-03",
  "speakers": [
    "Janu Verma"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/EZHmPIvmLCk/hqdefault.jpg",
  "title": "Transfer Learning in Natural Language Processing",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=EZHmPIvmLCk"
    }
  ]
}
