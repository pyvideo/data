{
  "description": "Manually running scripts to extract, transform and load data is a\ntrade-off with time, is tedious and cumbersome. The process of building\na data pipeline can be automated. Scripts to extract data can be\nscheduled using crontab. However, using crontab has its own drawbacks.\nOne major challenge is monitoring. Airflow is a platform to\nprogrammatically author, schedule and monitor workflows.\n\nToday, we are moving towards machine learning. Making predictions,\nfinding out insights based on data. For the same purpose, the initial\nstep is to have efficient processes in place which help us in collecting\ndata from various different data sources. Using traditional ways to\ncollect data is tedious and cumbersome. Manually running scripts to\nextract, transform and load data is a trade-off with time.\n\nTo make the process efficient. The data pipeline can be automated.\nScripts to extract data can be auto-scheduled using crontab. However,\nusing crontab has its own drawbacks. One major challenge comes in\nmonitoring. This is where an open source tool built by AirBnB\nengineering team - Apache airflow helps. Airflow is a platform to\nprogrammatically author, schedule and monitor workflows.\n\nThe talk aims at introducing the attendees to.\n\n1. Airflow - overview of the tool. Advantages, disadvantages\n2. Directed acyclic graph - Examples of directed acyclic graph and\n   directed cyclic graphs\n3. Operators a. Bash Operator b. Python Operator c. Email Operator\n4. Python context manager\n5. Examples\n6. Demo\n",
  "duration": 1544,
  "language": "eng",
  "recorded": "2019-08-03",
  "speakers": [
    "Mridu Bhatnagar"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/1hy6YwsVZaU/hqdefault.jpg",
  "title": "Automating Data Pipeline using Apache Airflow",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=1hy6YwsVZaU"
    }
  ]
}
