{
  "description": "Opacity is one of the biggest challenges in machine learning/deep\nlearning solutions in the real world. Any basic deep learning model can\ncontain dozens of hidden layers and millions of neurons interacting with\neach other. Explaining the Deep Learning model solutions can be a bit\nchallenging. Our proposal explain some Approaches that can help to make\nML/DL models more interpretable.\n\nModel Interpretability Background\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nData Science/AI models are still often perceived as a black box capable\nof performing magic. As we are solving more complex problems using\nadvanced algorithms, the situation is such that more sophisticated the\nmodel, lower is the explainability level.\n\nWithout a reasonable understanding of how DS/AI model works, real-world\nprojects rarely succeed. Also, business may not know the intricate\ndetails of how a model might work and as model will be making a lot of\ndecisions for them in the end, they do have a right to pose the\nquestion.\n\nA lot of real-world scenarios where biased models might have really\nadverse effects e.g. predicting potential criminals\n(https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-\nsentencing), judicial sentencing risk scores\n(https://www.propublica.org/article/making-algorithms-accountable),\ncredit scoring, fraud detection, health assessment, loan lending,\nself-driving.\n\nMany researchers are actively working on making DS/AI models\ninterpretable (Skater, ELI5, SHAP etc).\n\nWhy Model Interpretability is important?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDS/AI models are used to make critical decisions on behalf of business.\nFor the decisions taken by DS/AI models, business needs to cover these\nthree aspects as well:\n\n-  Fairness - How fair are the predictions? What drives model\n   predictions?\n-  Accountability - Why did the model take a certain decision?\n-  Transparency - How can we trust model predictions?\n\nHow to make models interpretable?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn order to make models interpretable, following approaches/techniques\ncan be used:\n\n1. Feature Importance\n2. Partial Dependence Plot\n3. SHAP Values\n4. LIME\n5. Skater\n\nLets have a look at these approaches/techniques one by one:\n\n1. Feature Importance\n^^^^^^^^^^^^^^^^^^^^^\n\nFor Machine Learning Models like XGBoost, Random Forest, Machine\nLearning Feature Importance helps Business Analysts drive Logical\nConclusion out of it.\n\nWe measure the importance of a feature by calculating the increase in\nthe model\u2019s prediction error after permuting the feature. A feature is\n\u201cimportant\u201d if shuffling its values increases the model error, because\nin this case the model relied on the feature for the prediction. A\nfeature is \u201cunimportant\u201d if shuffling its values leaves the model error\nunchanged, because in this case the model ignored the feature for the\nprediction.\n\n.. figure:: https://lh6.googleusercontent.com/6QlWI_TX3B40v5uvcwB3A0ADF3y4JDNUEJFtaRMCoCdn7QouTqB4M4bgTPzukoXT5PN4YAnphqqavM_yreeHCI1ObwYZqnHmeYn9AGhtkC-1zCmb9W55mhdqS66J3quq9DeRS8FE\n   :alt: \n\n2. Partial Dependence Plot\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPartial dependence plots show how a feature affects predictions. Partial\ndependence plots (PDP) show the dependence between the target response\nand a set of \u2018target\u2019 features, marginalizing over the values of all\nother features (the \u2018complement\u2019 features). Intuitively, we can\ninterpret the partial dependence as the expected target response as a\nfunction of the \u2018target\u2019 features.\n\n.. figure:: https://lh3.googleusercontent.com/SpyncU_BRXeMhocCaird59qXmIoLGISyPOQA1KEqj_IUHYxP58yu4yZuMwGL5C1VOWvHl_UOgvK7VgRzCuOh9OhAxqk7cZZodut9CaygiWWvxLcBYLFWQQ_L0iHMUugv5DrbA8Xc\n   :alt: \n\n3. SHAP (SHapley Additive exPlanations) Values\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSHAP Values break down a prediction to show the impact of each feature.\nThese are the scenarios where we need this technique:\n\n-  A model says a bank shouldn't loan someone money, and the bank is\n   legally required to explain the basis for each loan rejection\n-  A healthcare provider wants to identify what factors are driving each\n   patient's risk of some disease so they can directly address those\n   risk factors with targeted health interventions.\n-  |image0|\n\nWe predicted 0.7, whereas the base\\_value is 0.4979. Feature values\ncausing increased predictions are in pink, and their visual size shows\nthe magnitude of the feature's effect. Feature values decreasing the\nprediction are in blue. The biggest impact comes from Goal Scored being\n2. Though the ball possession value has a meaningful effect decreasing\nthe prediction.\n\nThe SHAP package has explainers for every type of model. -\nshap.DeepExplainer works with Deep Learning models. -\nshap.KernelExplainer works with all models, though it is slower than\nother Explainers and it offers an approximation rather than exact Shap\nvalues.\n\n4. LIME (Local Interpretable Model-Agnostic Explanations)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nLIME (https://github.com/marcotcr/lime) can be used on anything from a\npolynomial regression model to a deep neural network.\n\nLIME\u2019s approach is to perturb most of the features of a single\nprediction instance \u2014 essentially zeroing-out these features \u2014 and then\nto test the resulting output. By running this process repeatedly, LIME\nis able to determine a linear decision boundary for each feature\nindicating its predictive importance (e.g. which pixels contributed the\nmost to the classification of a specific image).\n\nInterpretation of Lime :--\n\n-  Local - Local refers to local fidelity - i.e., we want the\n   explanation to really reflect the behaviour of the classifier\n   \"around\" the instance being predicted.\n\n-  Interpretable - Lime explain output of Classifiers which are\n   interpretable by humans. For e.g. Representing words for a Model\n   which is built on word embeddings.\n\n-  Model Agnostic - Lime is able to explain a Machine Learning Model\n   without understanding it in deep.\n\n-  Explanation - Lime explanations are not too long so that it is\n   difficult for Humans to understand it.\n\n.. figure:: https://lh4.googleusercontent.com/JmXlS0qJNYOvbLlmA53X42_WIGHp9uzDCItBtGpmPM8YHqgqlYzJ077VU0EjNVna6LNZHvgFHRWry6c_CUMCZ_-%20WnoZh2F3RoLE4Xalh_aimWw8QDkLFPzxPYjLtCZ8Ws7DZzPcW\n   :alt: \n\n.. figure:: https://lh3.googleusercontent.com/g-nAKqqfemQR17DhBKzdYUDQJQYo7Q54Nzyf4rtTNInn8ZyI16l9VM8LmfaAclj40v5IhZHserrJY-%20qR-gA5_r6bwWlIat24sjdiuW085pkggHgrOgSbq_VQzZJnht-FyHChp9Zr\n   :alt: \n\n5. Skater\n^^^^^^^^^\n\nSkater is a Python library designed to demystify the inner workings of\ncomplex or black-box models. Skater uses a number of techniques,\nincluding partial dependence plots and local interpretable model\nagnostic explanation (LIME), to clarify the relationships between the\ndata a model receives and the outputs it produces.\n\n.. |image0| image:: https://lh5.googleusercontent.com/lWsT9o5da1242Caaqqj66lWpY9yND6vEy4_3eT4dY_5Juyysnv3ZE4etya20rQMGzJ5E5PgJNUP05lLQZCuDUiAC0dfPlWjwZq-1m2p8SBylGDytFYRQCSBilE6pBVl7kRdjcdpV\n\n",
  "duration": 1493,
  "language": "eng",
  "recorded": "2019-08-03",
  "speakers": [
    "Ankit Rathi",
    "Yatin Bhatia"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/bAoJnCeKFZA/hqdefault.jpg",
  "title": "Understanding Opacity in Machine Learning Models",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=bAoJnCeKFZA"
    }
  ]
}
