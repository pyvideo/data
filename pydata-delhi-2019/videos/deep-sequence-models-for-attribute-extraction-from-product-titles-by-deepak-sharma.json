{
  "description": "We are working on extracting attributes (brand, shape, color etc) from\nraw product descriptions. The text is short and noisy and highly\ncontextual and the labeling of attributes for training ML models is\ncostly. I discuss how we build a deep sequence CNN-BiLSTM-CRF model in\nPytorch to extract attributes from noisy text with minimum labeling\nusing an active learning approach.\n\nAt Clustr, I am working on converting raw product data available with\nSMEs to structured catalog. One of the key tasks of building catalogue\nis extracting attributes from raw product titles. Typical attributes\ninclude Brands, Color, Shape, measurement etc. Product titles are\nusually very short text describing the product without any significant\ngrammar. The titles are very dependent on the user which generates very\nnoisy text with abbreviations, spelling mistakes, omiitted text,\nimproper spaces, transliteration etc. The additional challenge is\navailability of labeled data to train a machine learning model on this.\nThe product data we receive is not labeled and labeling is a costly\nexcercise. I show how we built a deep sequence model with CNN, BiLSTM\nand CRF architecture and tuned it using active learning methods for this\ntask.\n\nI will discuss various deep sequence models combined with conditional\nrandom fields to label attributes from such text and outline pros and\ncons of different architectures. The model uses pretrained word\nembeddings. I will outtine some of the challenges of sparse tokens and\nnoise while building our domain specific word embeddings. A key aspect\nof the problem is the lack of labeled data and high cost of getting this\ndata. To minimize the cost of labeling I trained our model using an\nactive learning approach. The active labeling requires sampling\nstrategies such that minimum labeling can have maximum improvement in\nmodel performance. I implemented both model confidence based sampling\nand data coverage based sampling such that we are able to label examples\nwhich the model is least confident about and which are very different\nfrom the existing training examples. The active learning examples in a\nsingle training iteration were only about 1000 examples. Training models\nwith such few examples required us to be very careful about overfitting\nin training. I will also talk about how I regularized the model.\n\nTo rapidly iterate in experiments I created an experimental setup which\nallowed rapid changes and traceability. It was challenging to measure\nthe performance of the model and understand the limitations of the\nmodel. To do this more effectively, I tracked various metrics to measure\nthe performance of the model including various metrics relevant to the\nNamed Entity Tasks. These metrics were very informative in identifying\nthe gaps in the model. I will discuss these in detail.\n\nOverall this talk will provide audience a good in depth understanding of\nhow deep sequence models are built in Pytorch for challenging\ninformation extraction tasks. They will understand the pros and cons of\ndifferent architectures, things to keep in mind while tuning such deep\nmodels and how active learning is performed in deep sequence models.\n",
  "duration": 1565,
  "published_at": "2019-10-17T18:19:41.000Z",
  "recorded": "2019-08-04",
  "speakers": [
    "Deepak Sharma"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/zzcCP2ahBIc/hqdefault.jpg",
  "title": "Deep Sequence Models for Attribute Extraction from Product Titles",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=zzcCP2ahBIc"
    }
  ]
}
