{
  "description": "LIME, a model-agnostic AI framework, illuminates the path to local explainability, primarily for classification models. Delving into the theory underpinning LIME, we explore diverse use cases and its adaptability across various scenarios. Through practical examples, we showcase the breadth of applications for LIME. By the presentation's conclusion, you'll have gained insights into leveraging LIME to clarify individual prediction logic, leading to more accessible explanations.\n\nAlthough AI toolkits have simplified model implementation, understanding and interpreting these models remain challenging. With regulatory frameworks like the EU AI Act emphasizing explainability, the need for tools like LIME is paramount.\n\nThis presentation will provide an in-depth overview of LIME (Local Interpretable Model-agnostic Explanations), highlighting its utility in facilitating model comprehension. No prior expertise is assumed. Beginning with an explanation of LIME's theory and its practical implementation in Python, we'll then delve into diverse classification scenarios to showcase LIME's effectiveness. Additionally, we'll explore how the original LIME framework has been extended to handle time series data.",
  "duration": 1968,
  "language": "eng",
  "recorded": "2024-07-11",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://pydata.org/eindhoven2024/"
    }
  ],
  "speakers": [
    "Sanne van den Bogaart"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/9Hckg9gsBYs/maxresdefault.jpg",
  "title": "Explainable AI in the LIME-light",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=9Hckg9gsBYs"
    }
  ]
}
