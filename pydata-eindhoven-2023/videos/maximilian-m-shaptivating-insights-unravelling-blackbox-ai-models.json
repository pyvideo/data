{
  "description": "he advancements of (black box) artificial intelligence toolkits in recent years have made implementing AI models a commodity. However, model implementation is only the beginning during application development. Understanding, optimizing, and troubleshooting models is remaining as a constant challenge. In particular, the understanding (or explainability) of AI models is expected to become a requirement with the EU AI act, expected to pass this year.\n\nIn this presentation, SHAP (SHapley Additive exPlanations), a model agnostic AI explainability framework is explained using the example of a tabular classification problem in Python. First, we look at the theory behind SHAP and demonstrate the practical implementation in Python. Second, the usage of the SHAP framework is showcased for both global and local explainability. For this the filtering of bank transactions for suspicious activities is used as an example. It will be shown how SHAP was used to perform feature selection, understand the model\u2019s sensitivity to individual features and how single predictions can be explained. Last, a translation from SHAP to human readable output will be shown which was developed to explain the model predictions to the end user.\n\nTime breakdown:\n- General + domain introduction: 5 min\n- SHAP theory and Python framework: 5 min\n- Deep-dive into global and local explainability: 10 min\n- Converting SHAP values to human readable output: 5 min\n- Q&A: 5 min",
  "duration": 2102,
  "language": "eng",
  "recorded": "2023-11-30",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://web.archive.org/web/20240930133013/http://pydata.org/eindhoven2023"
    }
  ],
  "speakers": [
    "Maximilian Messmer"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/uswpei2GpLE/maxresdefault.jpg",
  "title": "SHAPtivating Insights: unravelling blackbox AI models",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=uswpei2GpLE"
    }
  ]
}
