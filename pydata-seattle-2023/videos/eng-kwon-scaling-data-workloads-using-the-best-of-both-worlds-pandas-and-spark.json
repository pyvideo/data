{
  "description": "www.pydata.org\n\nIt is indisputable that pandas is oftentimes the keystone element in any data wrangling and analysis workloads. However, the challenge is that pandas is not meant for big data processing. This presents data practitioners a dilemma: should we downsample data and lose information? Or should we explore a distributed processing framework to scale out data workloads? An example of a mainstream distributed processing tool is Apache Spark. However, this means data practitioners now have to learn a new language, PySpark. Not all is bleak though: pandas API on Spark provides pandas equivalent APIs in PySpark. It allows pandas users to transition from single-node to distributed environment, by just simply swapping the pandas package with pyspark.pandas.\n\nOn the other hand, existing PySpark users may wish to write their own custom user-defined functions (UDFs) that are not included in existing PySpark API. Pandas Function APIs, newly included in Spark 3.0+, allow users to apply arbitrary Python native functions, with pandas instances as the input and output against a PySpark dataframe. For instance, data scientists could use pandas function API to train a ML model based on each group of data using a single line of code.\n\nCo-presented by both a top open-source Apache Spark commiter and a hands-on data science consultant, this talk equips data analysts and scientists with the knowledge of scaling their data analysis workloads with implementation details and best practice guidance. Working knowledge of pandas, basic Spark, and machine learning is helpful.\n\nPyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R. \n\nPyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.\n\n00:00 Welcome!\n00:10 Help us add time stamps or captions to this video! See the description for details.\n\nWant to help add timestamps to our YouTube videos to help with discoverability? Find out more here: https://github.com/numfocus/YouTubeVideoTimestamps",
  "duration": 2168,
  "language": "eng",
  "recorded": "2023-04-26",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://pydata.org/seattle2023/"
    },
    {
      "label": "https://github.com/numfocus/YouTubeVideoTimestamps",
      "url": "https://github.com/numfocus/YouTubeVideoTimestamps"
    }
  ],
  "speakers": [
    "TODO"
  ],
  "tags": [
    "Education",
    "Julia",
    "NumFOCUS",
    "Opensource",
    "PyData",
    "Python",
    "Tutorial",
    "coding",
    "how to program",
    "learn",
    "learn to code",
    "python 3",
    "scientific programming",
    "software"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/knxbfJuC67I/maxresdefault.jpg",
  "title": "Eng & Kwon - Scaling data workloads using the best of both worlds: pandas and Spark",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=knxbfJuC67I"
    }
  ]
}
