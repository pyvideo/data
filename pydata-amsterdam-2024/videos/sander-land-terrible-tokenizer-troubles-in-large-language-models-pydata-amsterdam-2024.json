{
  "description": "www.pydata.org\n\nHuge amounts of resources are being spent training large language models in an end-to-end fashion. But did you know that at the bottom of all these models remains an important but often neglected component that converts text to numeric inputs? As a result of weaknesses in this \u2018tokenizer\u2019 component, some inputs can not be understood by language models, causing wild hallucinations, or worse.\nThis talk will cover some of our recent research in finding what text causes problems for a specific model, and show you how to break even the most advanced models.\n\nPyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R. \n\nPyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.\n\n00:00 Welcome!\n00:10 Help us add time stamps or captions to this video! See the description for details.\n\nWant to help add timestamps to our YouTube videos to help with discoverability? Find out more here: https://github.com/numfocus/YouTubeVideoTimestamps",
  "duration": 2057,
  "language": "eng",
  "recorded": "2024-09-18",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://web.archive.org/web/20240822042916/https://amsterdam.pydata.org/"
    },
    {
      "label": "https://github.com/numfocus/YouTubeVideoTimestamps",
      "url": "https://github.com/numfocus/YouTubeVideoTimestamps"
    }
  ],
  "speakers": [
    "Sander Land"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/WKUrGr5An-E/maxresdefault.jpg",
  "title": "Terrible tokenizer troubles in large language models",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=WKUrGr5An-E"
    }
  ]
}
