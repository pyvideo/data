{
  "description": "Modern deep learning model performance is very dependent on the choice\nof model hyperparameters, and the tuning process is a major bottleneck\nin the machine learning pipeline. In this talk, we will overview modern\nmethods for hyperparameter tuning and demonstrate how to use Tune, a\nscalable hyperparameter tuning library. Tune is completely open source\nat http://tune.io.\n\nThis talk will target intermediate to advanced data scientists and\nresearchers familiar with deep learning. The talk will first motivate\nthe need for advancements in hyperparameter tuning methods. The talk\nwill then overview standard methods for hyperparameter tuning: grid\nsearch, random search, and bayesian optimization. Then, we will motivate\nand discuss cutting edge methods for hyperparameter tuning:\nmulti-fidelity bayesian optimization, successive halving algorithms\n(HyperBand), and population-based training.\n\nThe talk will then present a overview of Tune, a scalable hyperparameter\ntuning system from the UC Berkeley RISELab, and demonstrate about how\nusers can leverage cutting edge hyperparameter tuning methods\nimplemented in Tune to quickly improve the performance of standard deep\nlearning models.\n",
  "duration": 2291,
  "published_at": "2019-12-23T21:03:56.000Z",
  "recorded": "2019-12-04",
  "speakers": [
    "Richard Liaw"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/10uz5U3Gy6E/hqdefault.jpg",
  "title": "A Guide to Modern Hyperparameter Tuning Algorithms",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=10uz5U3Gy6E"
    }
  ]
}
