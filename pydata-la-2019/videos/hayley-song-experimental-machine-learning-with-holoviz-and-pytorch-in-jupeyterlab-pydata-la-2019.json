{
  "description": "This tutorial introduces how to make your data exploration and neural\nnetwork training process more interactive and exploratory by using the\ncombination of JupyterLab, HoloViews, and PyTorch. I will first\nintroduce the basic concepts behind HoloViews, and walk through how to\nembellish each step of your machine learning workflow with HoloVie to\nemphasize the experimental nature of modeling.\n\n**Update** : Please visit `this\nrepo <https://github.com/cocoaaa/PyData-%20LA-2019>`__ for tutorial\nmaterials\n\n-  Subtitle: A guide through multi-class road detection on satellite\n   images with interactive visualization and explorative model building\n-  Author: Hayley Song (`[email\n   protected] </cdn-cgi/l/email-protection>`__)\n-  Category: step-by-step tutorial\n-  Prereq:\n\n   -  Basic understanding of visaulization with python (eg. previously\n      have used matplotlib.pyplot library)\n   -  | Basic understanding of neural network training process\n      | I'll give a brief overview of the workflow, assuming audiences'\n        previous experience with the following concepts\n\n   -  mini-batch training\n   -  forward-pass, backword-pass\n   -  gradient, gradient descent algorithm\n   -  classification, semantic segmentation\n   -  image as numpy ndarray\n\n-  Material distribution\n\n   -  All materials needed to follow the tutorial will be shared in a\n      self-containing GitHub repo, as well as a Binder environment\n   -  **Update** : Please visit `this\n      repo <https://github.com/cocoaaa/PyData-LA-2019>`__ for tutorial\n      materials\n   -  Links to extra resources will be provided as appropriate\n\nOverview\n--------\n\nThis tutorial introduces how to make your data exploration and model\nbuilding process more interactive and exploratory by using the\ncombination of JupyterLab, HoloViews, and PyTorch.\n`HoloViews <https://HoloViews.org/>`__ is a set of Python libraries that\noffers simple yet powerful visualization and GUI building tools which,\ntogether with other data analysis libraries (eg. ``pandas``,\n``geopandas``, ``numpy``) and machine learning framework (eg.\n``PyTorch``, ``Tensorflow``) can make your modeling procedure more\ninteractive and exploratory. I will start by introducing four core\nHoloViews libraries (Holoviews, GeoViews, Panel and Param) and\ndemonstrate basic examples on how we can essentially replace any\n\"Matplotlib.pyplot\" calls with equivalents in ``HoloViews``. You will\nsee how this opens up the possibilities to directly interact with your\nvisualization by eg. hovering over the graph to inspect values, querying\nRGB values of an image, or Lat/Lon values on your map.\n\nFollowing the introduction of the HoloViews libraries, I will\ndemonstrate how to embellish each step of your machine learning workflow\nwith HoloViews. First, you will learn to easily turn your PyTorch codes\ninto a simple GUI that encaptulates the state of your model (or\nalternatively, the state of your training session). This GUI explicitly\nexposes your model parameters and training hyperparameters (eg. learning\nrate, optimizer settings, batch size) as directly tunable parameters.\nCompared to conventional ways of specifying the hyperparameter settings\nwith the help of 'argparse' library or config files, this GUI approach\nfocuses on the experimental nature of modeling and integrates seamlessly\nwith Jupyter notebooks. After training a neural network model using our\nown GUI in the notebook, I will demonstrate how to understand the model\nby visualizing the intermediate layers with HoloViews and test the model\nwith test images directly sampled from HoloViews visualization.\n\nTo illustrate these steps, I will focus on the problem of classfying\ndifferent types of roads on satellite images, defined as a multi-class\nsemantic segmentation problem. Starting from the data exploration to the\ntrained model understanding, you will learn different ways to explore\nthe data and models by easily building simple GUIs in a Jupyter\nnotebook.\n\nIn summary, by the end of the talk you will have learned: - how to make\nyour data exploration more intuitive and experimental using HoloViews\nlibraries - how to turn your model script into a simple GUI that allows\ninteractive hyperparameter tuning and model exploration - how to monitor\nthe training process in realtime - how to quickly build a GUI tool to\ninspect the trained models in the same Jupyter notebook\n\nThe provided example codes will be a great starting point to experiment\nthese tools on your own datasets and tasks.\n\nOutline\n-------\n\nThis tutorial will consists of five main sections. I will first\nintroduce the basic concepts behind ``Holoviews/Geoviews`` and ``Panel``\nwhich are the main libraries we are going to use to add interactive\nexploration tools for data exploration and model training/evaluation,\nall in a single Jupyter notebook. This will take ~15 minutes. The rest\nof the tutorial will flow in the order of the general neural network\ntraining workflow, while integrating these libraries at each step. I\nwill leave the last <10 minutes for questions.\n\n-  Step 0: Introduction to ``Holoviews``/``Geoviews`` and ``Panel``\n   [15mins]\n-  Step 1: Explore your dataset with ``Holoviews``/``Geoviews`` [15mins]\n-  Step 2: Build an easily-configurable neural network model with\n   ``param`` [15mins]\n-  Step 3: Monitor your training process through an interactive GUI\n   [15mins]\n-  Step 4: Analyze your learned model on new images + Understand what\n   your model has learned by looking at intermediate feature maps with\n   ``Holoviews`` and ``Panel`` [15mins]\n-  Q/A [5~10 mins]\n\nStep 0: Introduction to ``HoloViews`` libraries\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn this introductory section, I will go over the basic concepts behind\nthe ``HoloViews`` libraries. I will provide simple examples that show\nhow we can replace any ``Matplotlib`` plot calls with equivalent calls\nin ``Holoviews/Geoviews`` with no hassle, and build easy tools to\ninteract with your data.\n\nStep 1: Explore your dataset\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe first step in building a machine learning model is to understand\nyour dataset. For the scope of this tutorial (ie.semantic segmentation\nof road types from satellite images), we will use the SpaceNet datasets.\nMore details on how to get the data as well as how the data are\ncollected and annotated can be found\n`here <https://spacenetchallenge.github.io/datasets/datasetHomePage.html>`__.\nThe original dataset is very large (>100GB) and requires a lot of\npreprocessing to be useful for training. For example, the RGB images are\n16bits of size 1300x1300, and the \"target\" roads are vector lines (as\nopposed to raster images), which means they need to be rasterized. I\nhave prepared a smaller sample dataset consisting of the RGB images\nconverted to 8bits and cropped to 520x520 size, as well as road buffers\nas rasters which can be easily used as the target images. I will share\nthe dataset to accompany my tutorial. The shared dataset will consists\nof input RGB images and target mask images. Each pixel of a target image\nwill contain one of the labels in {'highway', 'track', 'dirt', 'others'}\n(as ``uint8``).\n\nThe focus of this section is to show how to build a GUI-like\nvisualization of a satellite dataset within a Jupyter notebook using\n``Holoviews``/``Geoviews``. See Figure 1 (in the shared Google Drive)\nfor an example. Unlike a static plot (eg. one that is generated from\nMatplotlib), one can hover over the ``Holoviews`` plot to inspect the\nlabels at each pixel of the mask image or to check the lat/lon\nlocations. Furthermore I will show how you can trigger more complicated\ncomputations (eg. compute road length within a selected zone), while\ninteracting with the plot directly, eg. selecting a region by mouse\ndrag, clicking a lat/lon by mouse click.\n\nThe second example will show how this interactive plot can extended to\nincorporate external information (eg. roadlines from OpenStreetMap) to\neasily compare with your own dataset. See Figure 2 (in the shared Google\nDrive) for a snapshot of such tool. In this example, as you select\ndifferent RGB filenames (of your dataset), you have an option to click\non the 'click to download OSM' to download the corresponding region's\nOSM road data, and visualize it as an interactive map.\n\nStep 2: Monitor the training process\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn this section, I will show how to wrap around a ``PyTorch``'s NN model\nwith ``param``'s \\`Parametrized' class to expose its hyperparameters as\ntunable parameters. Using the GUI representation of the NN model, we can\ncontrol the (hyper)parameter configurations more intuitively, and study\ntheir effects. Its seamless integration into a Jupyter notebook\nfacilitates the experimental side of machine learning training pocess.\n\nStep 3: Interactively test your trained model on the new data\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nStep 4: Understand what the model has learned\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n\nI will conclude the tutorial by summarzing the main takeaways and\nproviding pointers to useful resources:\n\n-  General\n\n   -  Github repo for this talk\n   -  Link to HoloViews libraries\n   -  more: DataShader\n   -  PyTorch, torchvision\n\n-  Geospatial Data\n\n   -  remote sensing data: google-earth-engine\n   -  libraries: xarray, dash, rasterio, geopandas\n",
  "duration": 4391,
  "language": "eng",
  "published_at": "2019-12-23T21:02:16.000Z",
  "recorded": "2019-12-03",
  "speakers": [
    "Hayley Song"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/xdux2jwoNw4/hqdefault.jpg",
  "title": "Experimental Machine Learning with HoloViz and PyTorch in Jupyterlab",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=xdux2jwoNw4"
    }
  ]
}
