{
  "description": "Today, the computational limits of CPUs are being realized, and GPUs are\nbeing utilized to satisfy the compute demands of users. In the past,\nthis has meant low level programming in C/C++, but today there is a rich\necosystem of open source software with Python APIs and interfaces. This\ntalk will highlight the journey of developing open source software on\ntop of and integrating with this ecosystem.\n\n1. PyData Ecosystem\n\n   -  Pandas, Numpy, SciPy, SKLearn, Dask, Cython, etc.\n   -  Highly interoperable with everything standardizing around Numpy /\n      Pandas\n   -  Highly productive\n   -  Compute limited\n\n2. Apache Big Data Ecosystem\n\n   -  Spark, Beam, Flink, Hive, Impala, etc.\n   -  Semi interoperable but very technology dependent\n   -  Semi productive\n   -  Still compute limited\n\n3. GPUs\n\n   -  Thrust, CUB, NCCL, OpenUCX, etc.\n   -  Not very interoperable\n   -  Not productive\n   -  Not compute limited!\n\n4. Apache Arrow\n\n   -  Standards for memory layouts\n   -  Cross language compatible\n   -  Potential to bridge the PyData, Apache Big Data, and GPU\n      ecosystems!\n\n5. RAPIDS\n\n   -  Combining the compute of GPUs with the productivity of the PyData\n      ecosystem with the integration and interoperability of Apache\n      Arrow\n   -  Built on top of OSS C/C++ GPU Ecosystem: Thrust, CUB, NCCL,\n      OpenUCX\n   -  Integrated with OSS Python GPU Ecosystem: Numba, CuPy, PyTorch\n   -  Built on top of and integrated with OSS PyData Ecosystem: Pandas,\n      Numpy, Dask, Cython\n\n6. Ecosystem Interoperability\n\n   -  Standards / Protocols\n   -  Numpy ``__array_function__`` protocol\n   -  ``__cuda_array_interface__`` protocol\n   -  DLPack\n   -  User Experience\n   -  Follow the same Python APIs that users are comfortable,\n      productive, and happy with\n   -  Performance\n   -  Deliver 10-1000x the performance with nearly zero code change\n   -  Scaling\n   -  Scale the same way as existing PyData ecosystem with Dask\n   -  Improve Dask for everyone with lower level communication\n      acceleration\n\n7. Struggles\n\n   -  CI\n   -  Travis-CI doesn\u2019t cut it for GPUs and no easy to use off the shelf\n      alternative\n   -  Programming Paradigm Mindset\n   -  Thinking in terms of vectorized operations instead of loops /\n      iterations\n   -  Amdahl\u2019s Law\n   -  New bottlenecks that we didn\u2019t previously worry about\n\n8. Conclusion\n9. Q/A\n",
  "duration": 2529,
  "published_at": "2019-12-23T21:04:51.000Z",
  "recorded": "2019-12-05",
  "speakers": [
    "Dante Gama Dessavre"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/y9Z_Vm84O4Y/hqdefault.jpg",
  "title": "Open Source is Better Together: GPU Python Libraries Unite",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=y9Z_Vm84O4Y"
    }
  ]
}
