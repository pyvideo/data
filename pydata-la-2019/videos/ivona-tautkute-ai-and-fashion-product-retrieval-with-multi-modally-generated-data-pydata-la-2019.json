{
  "description": "The talk will cover generative modeling for multimodal input (image and\ntext) in the context of product retrieval in fashion/e-commerce.\n\nThe presentation will include examples of applying generative (GAN)\narchitectures for image generation with multimodal query using models\nderived from Conditional GAN, StackGAN, AttnGAN and others.\n\nRetrieving products from large databases and finding items of particular\ninterest for the user is a topic of ongoing research. Moving further\nfrom text search, tag based search and image search, there is still a\nlot of ambiguity when visual and textual features need to be merged.\nText query might compliment an image (\"I want sport shoes like these in\nthe image, produced by XXX, wide fit and comfortable\") or might\nrepresent a difference from image query (\"I want a dress like that in\nthe picture, only with shorter sleeves\").\n\nTalk outline:\n\n-  Use cases in e-commerce and fashion\n-  Current methods for learning multimodal embedding (VSE, Multimodal\n   Siamese Networks)\n-  Intro to GAN architectures that take latent representation as an\n   input (we can influence what we generate, yeah!)\n-  How do you feed multimodal input into GAN\n-  Results and comparison\n",
  "duration": 1494,
  "language": "eng",
  "published_at": "2019-12-24T02:59:41.000Z",
  "recorded": "2019-12-05",
  "speakers": [
    "Ivona Tautkute"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/w079bCwSaKQ/hqdefault.jpg",
  "title": "AI meets Fashion for product retrieval with multi-modally generated data",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=w079bCwSaKQ"
    }
  ]
}
