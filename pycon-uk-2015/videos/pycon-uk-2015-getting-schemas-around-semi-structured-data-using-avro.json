{
  "description": "In this world of big-data where we are producing data of enormous\nvariety, velocity, volume and the nature being semi-structured we need\nto put some rules around data being collected which are not rigid yet\nallow efficient management of it right from collection layer.\n\nMy talk is based on first-hand experience that me and my team had while\nwriting frameworks and data pipelines , we like anyone else started with\ncollecting data in JSON format. But soon started to run into problems\nbecause of systems which were writing the data were abusing the\nflexibility.\n\nMore importantly how we can leverage Python with this and makes it\ninteroperable with various Big Data techologies.\n\nSchemas / Serialization is a layer not many people talk about , but in\nmy experience it is very useful and one can benefit from it in many many\nways.\n\nThe agenda of the talk is as follows :\n\n1. Need of a schema / serialization.\n2. Avro and why is it so awesome.\n3. Demo of creating some schemas and showcasing the features.\n4. How Avro serialized data can be used across different layers / tools\n   in the BigData pipeline. Be it Realtime processing system like Kafka\n   with Storm or Batch processing system like Hadoop, Hive etc.\n5. Setting up your AVRO schema repository to help people create,\n   distrubute and manage schemas. A pure Django based implementation.\n6. Do's and Don'ts .\n\nWho should Attend :\n\nI plan to keep it beginner friendly, any one who has been or is doing\ndata collection at any level can benefit from this talk.",
  "duration": 1416,
  "recorded": "2015-09-20",
  "speakers": [
    "Konark Modi"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/GTBeWRTJJJE/hqdefault.jpg",
  "title": "Getting schemas around semi-structured data using Avro",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=GTBeWRTJJJE"
    }
  ]
}
