{
  "description": "Title High-Performance Distributed Tensorflow: Request Batching and Model Post-Processing Optimizations\n\nFilmed at PyData London 2017\n\nDescription\nIn this completely demo-based talk, Chris will demonstrate various techniques to post-process and optimize trained Tensorflow AI models to reduce deployment size and increase prediction performance.\n\nAbstract\nIn this completely demo-based talk, Chris will demonstrate various techniques to post-process and optimize trained Tensorflow AI models to reduce deployment size and increase prediction performance.\n\nFirst, we'll use various techniques such as 8-bit quantization, weight-rounding, and batch-normalization folding, we will simplify the path of forward propagation and prediction.\n\nNext, we'll loadtest and compare our optimized and unoptimized models - in addition to enabling and disabling request batching.\n\nLast, we'll dive deep into Google's Tensorflow Graph Transform Tool to build custom model optimization functions.",
  "duration": 2154,
  "language": "eng",
  "recorded": "2017-05-06",
  "speakers": [
    "Chris Fregly"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/Bd-sSJC6UMM/hqdefault.jpg",
  "title": "High-Performance Distributed Tensorflow: Request Batching and Model Post-Processing Optimizations",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=Bd-sSJC6UMM"
    }
  ]
}
