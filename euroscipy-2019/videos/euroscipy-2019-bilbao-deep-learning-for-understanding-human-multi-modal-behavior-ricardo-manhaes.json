{
  "description": "| Multimedia automatic learning has drawn attention from companies and\n  governments for a significant number of applications for automated\n  recommendations, classification, and human brain understatement. In\n  recent years, and an increased amount of research has explored using\n  deep neural networks for multimedia related tasks.\n| Some government security and surveillance applications are automated\n  detections of illegal and violent behaviors, child pornography and\n  traffic infractions. Companies worldwide are looking for content-based\n  recommendation systems that can personalize clients consumption and\n  interactions by understanding the human perception of memorability,\n  interestingness, attractiveness, aesthetics. For these fields like\n  event detection, multimedia affect and perceptual analysis are turning\n  towards Artificial Neural Networks. In this talk, I will present the\n  theory behind multi-modal fusion using deep learning and some open\n  challenges and their state-of-the-art.\n\nMulti-modal sources of information are the next big step for AI. In this\ntalk, I will present the use of deep learning techniques for automated\nmulti-modal applications and some open benchmarks.\n",
  "duration": 870,
  "language": "eng",
  "published_at": "2020-03-06T12:30:17.000Z",
  "recorded": "2019-09-05",
  "speakers": [
    "Ricardo Manh\u00e3es Savii"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/vL-Tk4f1fg0/hqdefault.jpg",
  "title": "Deep Learning for Understanding Human Multi-modal Behavior",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=vL-Tk4f1fg0"
    }
  ]
}
