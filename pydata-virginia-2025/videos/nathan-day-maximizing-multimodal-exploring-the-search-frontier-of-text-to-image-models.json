{
  "description": "Text-to-Image models, like CLIP, have brought us into a new frontier of visual search. Whether it's searching by circling a section of a photo or powering image generators like Dalle-E the gap between pixels and tokens has never been smaller. This talk discusses how we are improving search and empowering designers with these models at Eezy, a stock art marketplace.",
  "duration": 1549,
  "language": "eng",
  "recorded": "2025-04-18",
  "related_urls": [
    {
      "label": "Conference Website",
      "url": "https://pydata.org/virginia2025"
    },
    {
      "label": "https://github.com/numfocus/YouTubeVideoTimestamps",
      "url": "https://github.com/numfocus/YouTubeVideoTimestamps"
    }
  ],
  "speakers": [
    "Nathan Day"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/Xzd25tbKFLA/maxresdefault.jpg",
  "title": "Maximizing Multimodal: Exploring the search frontier of text-to-image models",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=Xzd25tbKFLA"
    }
  ]
}
